{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning - Practical 4 - Deep Learning VS Trees\n\n\nNames: {YOUR NAMES}  \nSummer Term 2023   \nDue Date: Tuesday, June 13, 2pm","metadata":{"id":"bc713cda"}},{"cell_type":"markdown","source":"In this practical we are going to use a tabular dataset. We will test two different approaches - forests and neural networks and compare performance. We are also going to learn how to make trees interpretable.","metadata":{"id":"0e394c4e"}},{"cell_type":"markdown","source":"To prepare this tutorial we used [this paper](https://arxiv.org/pdf/2207.08815.pdf) with its [repository](https://github.com/LeoGrin/tabular-benchmark).\n\nFor explained variance in trees, you can read more [here](https://scikit-learn.org/0.15/auto_examples/ensemble/plot_gradient_boosting_regression.html#example-ensemble-plot-gradient-boosting-regression-py).\n","metadata":{"id":"72933ee7"}},{"cell_type":"code","source":"%matplotlib inline\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pickle\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import recall_score, precision_score, accuracy_score\nfrom sklearn.metrics import classification_report\n\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset, Subset\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torchvision.utils import make_grid\ntorch.manual_seed(42) # Set manual seed","metadata":{"id":"218de310","outputId":"93864191-1b7e-479f-bfd3-0759b4847266","execution":{"iopub.status.busy":"2023-06-12T16:03:35.451337Z","iopub.execute_input":"2023-06-12T16:03:35.451588Z","iopub.status.idle":"2023-06-12T16:03:41.678119Z","shell.execute_reply.started":"2023-06-12T16:03:35.451564Z","shell.execute_reply":"2023-06-12T16:03:41.677028Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c3db50f6650>"},"metadata":{}}]},{"cell_type":"code","source":"# DO NOT CHANGE\nuse_cuda = True\nuse_cuda = False if not use_cuda else torch.cuda.is_available()\ndevice = torch.device('cuda:0' if use_cuda else 'cpu')\ntorch.cuda.get_device_name(device) if use_cuda else 'cpu'\nprint('Using device', device)","metadata":{"id":"3bb2c837","execution":{"iopub.status.busy":"2023-06-12T16:03:45.174588Z","iopub.execute_input":"2023-06-12T16:03:45.175392Z","iopub.status.idle":"2023-06-12T16:03:45.292077Z","shell.execute_reply.started":"2023-06-12T16:03:45.175348Z","shell.execute_reply":"2023-06-12T16:03:45.290820Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using device cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load, clean and split the tabular dataset","metadata":{"id":"8e24771e"}},{"cell_type":"markdown","source":"We use the preprocessing pipeline from [Grinsztajn, 2022](https://arxiv.org/pdf/2207.08815.pdf).\n\n**No missing data**    \n\nRemove all rows containing at least one missing entry.    \n\n*In practice people often do not remove rows with missing values but try to fill missing values in a column with the mean or median values for numerical data and mode or median values for categorical data. Sometimes even simple prediction models are used to fill in the gaps but we will remove rows or columns with missing values for the sake of simplicity*\n\n**Balanced classes**   \n\nFor classification, the target is binarised if there are multiple classes, by taking the two most numerous classes, and we keep half of samples in each class.\n\n**Low cardinality categorical features**   \n\nRemove categorical features with more than 20 items. \n\n**High cardinality numerical features**   \n\nRemove numerical features with less than 10 unique values. Convert numerical features with 2 unique values to categorical.","metadata":{"id":"61d9f9e1"}},{"cell_type":"markdown","source":"**Data description:**  \nData reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979. This version includes data up to 2015. We will try to predict the sex of the driver based on the data provided.","metadata":{"id":"Gyi3ep5EtiIf"}},{"cell_type":"code","source":"## In case you have any issues with loading the pickle file\n## check that your pandas version is 1.4.1\n## or just simply run:\n## pip install pandas==1.4.1\n\nwith open('/kaggle/input/adopted-road-safety/adopted_road_safety.pkl', 'rb') as f:\n    dataset = pickle.load(f)","metadata":{"id":"bbc76543","execution":{"iopub.status.busy":"2023-06-12T16:03:49.996972Z","iopub.execute_input":"2023-06-12T16:03:49.997349Z","iopub.status.idle":"2023-06-12T16:03:51.164495Z","shell.execute_reply.started":"2023-06-12T16:03:49.997321Z","shell.execute_reply":"2023-06-12T16:03:51.163301Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"id":"288e7151","execution":{"iopub.status.busy":"2023-06-12T16:03:53.730219Z","iopub.execute_input":"2023-06-12T16:03:53.730599Z","iopub.status.idle":"2023-06-12T16:03:54.081936Z","shell.execute_reply.started":"2023-06-12T16:03:53.730572Z","shell.execute_reply":"2023-06-12T16:03:54.080982Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"       Accident_Index  Vehicle_Reference_df_res  Vehicle_Type  \\\n0       201501BS70001                         1          19.0   \n1       201501BS70002                         1           9.0   \n2       201501BS70004                         1           9.0   \n3       201501BS70005                         1           9.0   \n4       201501BS70008                         1           1.0   \n...               ...                       ...           ...   \n363238  2015984141415                        13           9.0   \n363239  2015984141415                        13           9.0   \n363240  2015984141415                        13           9.0   \n363241  2015984141415                        13           9.0   \n363242  2015984141415                        13           9.0   \n\n        Towing_and_Articulation  Vehicle_Manoeuvre  \\\n0                           0.0                9.0   \n1                           0.0                9.0   \n2                           0.0                9.0   \n3                           0.0                9.0   \n4                           0.0               18.0   \n...                         ...                ...   \n363238                      0.0               18.0   \n363239                      0.0               18.0   \n363240                      0.0               18.0   \n363241                      0.0               18.0   \n363242                      0.0               18.0   \n\n        Vehicle_Location-Restricted_Lane  Junction_Location  \\\n0                                    0.0                8.0   \n1                                    0.0                8.0   \n2                                    0.0                2.0   \n3                                    0.0                2.0   \n4                                    0.0                8.0   \n...                                  ...                ...   \n363238                               0.0                0.0   \n363239                               0.0                0.0   \n363240                               0.0                0.0   \n363241                               0.0                0.0   \n363242                               0.0                0.0   \n\n        Skidding_and_Overturning  Hit_Object_in_Carriageway  \\\n0                            0.0                        0.0   \n1                            0.0                        0.0   \n2                            0.0                        0.0   \n3                            0.0                        0.0   \n4                            0.0                        0.0   \n...                          ...                        ...   \n363238                       0.0                        0.0   \n363239                       0.0                        0.0   \n363240                       0.0                        0.0   \n363241                       0.0                        0.0   \n363242                       0.0                        0.0   \n\n        Vehicle_Leaving_Carriageway  ...  Age_Band_of_Casualty  \\\n0                               0.0  ...                   7.0   \n1                               0.0  ...                   5.0   \n2                               0.0  ...                   6.0   \n3                               0.0  ...                   2.0   \n4                               0.0  ...                   8.0   \n...                             ...  ...                   ...   \n363238                          5.0  ...                   1.0   \n363239                          5.0  ...                   5.0   \n363240                          5.0  ...                   4.0   \n363241                          5.0  ...                   6.0   \n363242                          5.0  ...                   4.0   \n\n        Casualty_Severity  Pedestrian_Location  Pedestrian_Movement  \\\n0                       3                  5.0                  1.0   \n1                       3                  9.0                  9.0   \n2                       3                  1.0                  3.0   \n3                       3                  5.0                  1.0   \n4                       2                  0.0                  0.0   \n...                   ...                  ...                  ...   \n363238                  3                  0.0                  0.0   \n363239                  3                  0.0                  0.0   \n363240                  3                  0.0                  0.0   \n363241                  3                  0.0                  0.0   \n363242                  3                  0.0                  0.0   \n\n       Car_Passenger  Bus_or_Coach_Passenger  \\\n0                0.0                     0.0   \n1                0.0                     0.0   \n2                0.0                     0.0   \n3                0.0                     0.0   \n4                0.0                     0.0   \n...              ...                     ...   \n363238           2.0                     0.0   \n363239           0.0                     0.0   \n363240           0.0                     0.0   \n363241           0.0                     0.0   \n363242           0.0                     0.0   \n\n        Pedestrian_Road_Maintenance_Worker  Casualty_Type  \\\n0                                      2.0              0   \n1                                      2.0              0   \n2                                      2.0              0   \n3                                      2.0              0   \n4                                      0.0              1   \n...                                    ...            ...   \n363238                                 0.0              9   \n363239                                 0.0              9   \n363240                                 0.0              9   \n363241                                 0.0              9   \n363242                                 0.0              9   \n\n        Casualty_Home_Area_Type  Casualty_IMD_Decile  \n0                           NaN                  NaN  \n1                           1.0                  3.0  \n2                           1.0                  6.0  \n3                           1.0                  2.0  \n4                           1.0                  3.0  \n...                         ...                  ...  \n363238                      1.0                  NaN  \n363239                      1.0                  2.0  \n363240                      2.0                  5.0  \n363241                      3.0                  NaN  \n363242                      1.0                  4.0  \n\n[363243 rows x 67 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accident_Index</th>\n      <th>Vehicle_Reference_df_res</th>\n      <th>Vehicle_Type</th>\n      <th>Towing_and_Articulation</th>\n      <th>Vehicle_Manoeuvre</th>\n      <th>Vehicle_Location-Restricted_Lane</th>\n      <th>Junction_Location</th>\n      <th>Skidding_and_Overturning</th>\n      <th>Hit_Object_in_Carriageway</th>\n      <th>Vehicle_Leaving_Carriageway</th>\n      <th>...</th>\n      <th>Age_Band_of_Casualty</th>\n      <th>Casualty_Severity</th>\n      <th>Pedestrian_Location</th>\n      <th>Pedestrian_Movement</th>\n      <th>Car_Passenger</th>\n      <th>Bus_or_Coach_Passenger</th>\n      <th>Pedestrian_Road_Maintenance_Worker</th>\n      <th>Casualty_Type</th>\n      <th>Casualty_Home_Area_Type</th>\n      <th>Casualty_IMD_Decile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>201501BS70001</td>\n      <td>1</td>\n      <td>19.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>7.0</td>\n      <td>3</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>201501BS70002</td>\n      <td>1</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>5.0</td>\n      <td>3</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>201501BS70004</td>\n      <td>1</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>6.0</td>\n      <td>3</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>201501BS70005</td>\n      <td>1</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>201501BS70008</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>8.0</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>363238</th>\n      <td>2015984141415</td>\n      <td>13</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>363239</th>\n      <td>2015984141415</td>\n      <td>13</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>5.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>363240</th>\n      <td>2015984141415</td>\n      <td>13</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>2.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>363241</th>\n      <td>2015984141415</td>\n      <td>13</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>6.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>3.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>363242</th>\n      <td>2015984141415</td>\n      <td>13</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>1.0</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>363243 rows × 67 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"target_column = 'Sex_of_Driver'\ntest_size = 0.2\nrandom_state = 42","metadata":{"id":"fAWW5kChD6MU","execution":{"iopub.status.busy":"2023-06-12T16:03:55.876974Z","iopub.execute_input":"2023-06-12T16:03:55.877446Z","iopub.status.idle":"2023-06-12T16:03:55.883057Z","shell.execute_reply.started":"2023-06-12T16:03:55.877409Z","shell.execute_reply":"2023-06-12T16:03:55.882064Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def remove_nans(df):\n    '''\n    this function removes rows with nans \n    '''\n    df = df.dropna()\n        \n    # TODO\n    return df\n\n\ndef numerical_to_categorical(df, n=2, ignore=[target_column]):\n    '''\n    change the type of the column to categorical \n    if it has <= n unique values\n    '''\n    \n    for col in df.columns:\n        if col not in ignore:\n            count = df[col].unique().size\n            if count<=n:\n                df[col] = df[col].astype(\"category\")\n            \n    return df\n\n\ndef remove_columns_by_n(df, n=10, condition=np.number, direction='less', \n                        ignore=[target_column]):\n    '''\n   \n    Remove columns with more or less than n unique values. \n    Usually it makes sense to apply this function to columns with categorical values (see below where it is called).\n    With the default values we remove all numerical columns which have less than 10 unique values (except for the target_column).\n    '''\n    # TODO\n    if direction=='less':\n        for col in df.columns:\n            if col not in ignore:\n                count = df[col].unique().size\n                if count < n :\n                    df = df.drop(col, axis = 1)\n    else:\n        for col in df.columns:\n            if col not in ignore:\n                count = df[col].unique().size\n                if count > n :\n                    df = df.drop(col, axis = 1)\n        \n        \n            \n    return df","metadata":{"id":"3d77435f","execution":{"iopub.status.busy":"2023-06-12T16:03:57.951894Z","iopub.execute_input":"2023-06-12T16:03:57.952837Z","iopub.status.idle":"2023-06-12T16:03:57.963775Z","shell.execute_reply.started":"2023-06-12T16:03:57.952792Z","shell.execute_reply":"2023-06-12T16:03:57.962782Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = dataset\ndf = remove_nans(df)\ndf = numerical_to_categorical(df, n=2, ignore=[target_column])\ndf = remove_columns_by_n(df, n=10, condition=np.number, direction='less', \n                         ignore=[target_column])\ndf = remove_columns_by_n(df, n=40, condition='category', direction='more', \n                         ignore=[target_column])\nassert not df.isna().any().any(), 'There are still nans in the dataframe'\n\ndf","metadata":{"id":"9033619f","execution":{"iopub.status.busy":"2023-06-12T16:04:00.859968Z","iopub.execute_input":"2023-06-12T16:04:00.861127Z","iopub.status.idle":"2023-06-12T16:04:01.646485Z","shell.execute_reply.started":"2023-06-12T16:04:00.861078Z","shell.execute_reply":"2023-06-12T16:04:01.645330Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/3679314217.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[col] = df[col].astype(\"category\")\n/tmp/ipykernel_28/3679314217.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[col] = df[col].astype(\"category\")\n/tmp/ipykernel_28/3679314217.py:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[col] = df[col].astype(\"category\")\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        Vehicle_Type  Vehicle_Manoeuvre  Vehicle_Location-Restricted_Lane  \\\n2                9.0                9.0                               0.0   \n6                3.0               18.0                               0.0   \n7               19.0                6.0                               0.0   \n8                9.0                9.0                               0.0   \n12               9.0               13.0                               0.0   \n...              ...                ...                               ...   \n328127           9.0                5.0                               0.0   \n328128           9.0               18.0                               0.0   \n328129           9.0               18.0                               0.0   \n328130           9.0                3.0                               0.0   \n328131           9.0                3.0                               0.0   \n\n        Hit_Object_in_Carriageway  Hit_Object_off_Carriageway Sex_of_Driver  \\\n2                             0.0                         0.0           1.0   \n6                             0.0                         0.0           1.0   \n7                             0.0                         0.0           1.0   \n8                             0.0                         0.0           1.0   \n12                            0.0                         0.0           2.0   \n...                           ...                         ...           ...   \n328127                        0.0                         0.0           2.0   \n328128                        0.0                         0.0           1.0   \n328129                        0.0                         0.0           1.0   \n328130                        0.0                         0.0           2.0   \n328131                        0.0                         0.0           2.0   \n\n        Age_Band_of_Driver  Police_Force  Number_of_Casualties  \\\n2                      6.0           1.0                   1.0   \n6                      7.0           1.0                   1.0   \n7                      7.0           1.0                   1.0   \n8                      7.0           1.0                   1.0   \n12                     7.0           1.0                   1.0   \n...                    ...           ...                   ...   \n328127                10.0          55.0                   2.0   \n328128                 4.0          55.0                   2.0   \n328129                 4.0          55.0                   2.0   \n328130                 7.0          55.0                   2.0   \n328131                 7.0          55.0                   2.0   \n\n        Casualty_Reference  Age_Band_of_Casualty  Pedestrian_Location  \\\n2                        1                   6.0                  1.0   \n6                        1                   7.0                  0.0   \n7                        1                   7.0                  0.0   \n8                        1                   7.0                  0.0   \n12                       1                   9.0                  0.0   \n...                    ...                   ...                  ...   \n328127                   2                   7.0                  0.0   \n328128                   1                   4.0                  0.0   \n328129                   2                   7.0                  0.0   \n328130                   1                   4.0                  0.0   \n328131                   2                   7.0                  0.0   \n\n        Pedestrian_Movement  Casualty_Type  Casualty_IMD_Decile  \n2                       3.0              0                  6.0  \n6                       0.0              3                  5.0  \n7                       0.0              3                  5.0  \n8                       0.0              1                  8.0  \n12                      0.0              5                  2.0  \n...                     ...            ...                  ...  \n328127                  0.0              9                  6.0  \n328128                  0.0              9                 10.0  \n328129                  0.0              9                  6.0  \n328130                  0.0              9                 10.0  \n328131                  0.0              9                  6.0  \n\n[96326 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Vehicle_Type</th>\n      <th>Vehicle_Manoeuvre</th>\n      <th>Vehicle_Location-Restricted_Lane</th>\n      <th>Hit_Object_in_Carriageway</th>\n      <th>Hit_Object_off_Carriageway</th>\n      <th>Sex_of_Driver</th>\n      <th>Age_Band_of_Driver</th>\n      <th>Police_Force</th>\n      <th>Number_of_Casualties</th>\n      <th>Casualty_Reference</th>\n      <th>Age_Band_of_Casualty</th>\n      <th>Pedestrian_Location</th>\n      <th>Pedestrian_Movement</th>\n      <th>Casualty_Type</th>\n      <th>Casualty_IMD_Decile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>19.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>9.0</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>328127</th>\n      <td>9.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>10.0</td>\n      <td>55.0</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>328128</th>\n      <td>9.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>55.0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>328129</th>\n      <td>9.0</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>55.0</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>328130</th>\n      <td>9.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>7.0</td>\n      <td>55.0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>328131</th>\n      <td>9.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>7.0</td>\n      <td>55.0</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>6.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>96326 rows × 15 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# TODO : make train-test split from the dataframe using the parameters above\n# expected results variable names - train_X, test_X, train_y, test_y\n\nX = df.drop(target_column, axis = 1)\nY = df[target_column]\n\ntrain_X, test_X, train_y, test_y  = train_test_split(X, Y, test_size=test_size, random_state=random_state)","metadata":{"id":"22348559","execution":{"iopub.status.busy":"2023-06-12T16:04:04.418798Z","iopub.execute_input":"2023-06-12T16:04:04.419169Z","iopub.status.idle":"2023-06-12T16:04:04.443954Z","shell.execute_reply.started":"2023-06-12T16:04:04.419142Z","shell.execute_reply":"2023-06-12T16:04:04.443054Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**TODO :**  \n\n* Did you split the dataset in a stratified manner or not? Why?\n* How did the dataset dimensions change after preprocessing?\n* How many unique values are in the response variable? ","metadata":{"id":"77f002d6"}},{"cell_type":"markdown","source":"## Task 1: Create a GradientBoostingClassifier","metadata":{"id":"f309f4d6"}},{"cell_type":"code","source":"## TODO : define the GradientBoostingClassifier, \n## train it on the train set and predict on the test set\nGBC = GradientBoostingClassifier()\n\n","metadata":{"id":"cxbPa0evFrRF","execution":{"iopub.status.busy":"2023-06-12T15:56:07.159300Z","iopub.execute_input":"2023-06-12T15:56:07.159640Z","iopub.status.idle":"2023-06-12T15:56:07.163976Z","shell.execute_reply.started":"2023-06-12T15:56:07.159612Z","shell.execute_reply":"2023-06-12T15:56:07.162956Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"## TODO : print  accuracy, precision, recall\n## Hint : use functions from sklearn metrics\nGBC.fit(train_X, train_y)\npredictions = GBC.predict(test_X)\naccuracy = accuracy_score(test_y, predictions)\nprecision = precision_score(test_y, predictions, average='weighted')\nrecall = recall_score(test_y, predictions, average='weighted')\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\n\n","metadata":{"id":"YZ5j6BMKFrcJ","execution":{"iopub.status.busy":"2023-06-12T14:01:10.608723Z","iopub.execute_input":"2023-06-12T14:01:10.609182Z","iopub.status.idle":"2023-06-12T14:01:26.177589Z","shell.execute_reply.started":"2023-06-12T14:01:10.609108Z","shell.execute_reply":"2023-06-12T14:01:26.176476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TODO : Write a function which iterates over trees_amount, \n## train a classifier with a specified amount of trees and print accuracy, precision, and recall.\n## Note: the calculations may take several minutes (depending on the computer efficiency).\n\ndef trees_amount_exploration(train_X, train_y, test_X, test_y, trees_amount=[1, 20, 50, 100]):\n    #TODO\n    for trees in trees_amount:\n        model = GradientBoostingClassifier(n_estimators=trees)\n        model.fit(train_X, train_y)\n        predictions = model.predict(test_X)\n        accuracy = accuracy_score(test_y, predictions)\n        precision = precision_score(test_y, predictions, average='weighted')\n        recall = recall_score(test_y, predictions, average='weighted')\n        \n        print('trees_amount:', trees, '\\n')\n        \n        print('Accuracy:', accuracy)\n        print('Precision:', precision)\n        print('Recall:', recall, '\\n \\n')\n        \n","metadata":{"id":"598f0ffa","execution":{"iopub.status.busy":"2023-06-12T15:13:48.271644Z","iopub.execute_input":"2023-06-12T15:13:48.271991Z","iopub.status.idle":"2023-06-12T15:13:48.280845Z","shell.execute_reply.started":"2023-06-12T15:13:48.271961Z","shell.execute_reply":"2023-06-12T15:13:48.278968Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"trees_amount_exploration(train_X, train_y, test_X, test_y)","metadata":{"id":"qcnxg4InEkRl","execution":{"iopub.status.busy":"2023-06-08T19:35:42.343285Z","iopub.execute_input":"2023-06-08T19:35:42.343695Z","iopub.status.idle":"2023-06-08T19:36:11.170497Z","shell.execute_reply.started":"2023-06-08T19:35:42.343665Z","shell.execute_reply":"2023-06-08T19:36:11.169285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TODO : Write a function which iterates over the learning rate, \n## train a classifier with a specified amount of trees and print accuracy, precision, and recall.\n## Note: the calculations may take several minutes (depending on the computer efficiency).\n\ndef learning_rate_exploration(train_X, train_y, test_X, test_y, learning_rates = [0.1, 0.2, 0.3, 0.4, 0.5], trees_amount=100):\n    #TODO\n    for learning_rate in learning_rates:\n        model = GradientBoostingClassifier(learning_rate=learning_rate)\n        model.fit(train_X, train_y)\n        predictions = model.predict(test_X)\n        \n        accuracy = accuracy_score(test_y, predictions)\n        precision = precision_score(test_y, predictions, average='weighted')\n        recall = recall_score(test_y, predictions, average='weighted')\n        \n        print('Learning rate:', learning_rate, '\\n')\n        \n        print('Accuracy:', accuracy)\n        print('Precision:', precision)\n        print('Recall:', recall, '\\n \\n')","metadata":{"execution":{"iopub.status.busy":"2023-06-12T15:13:51.189674Z","iopub.execute_input":"2023-06-12T15:13:51.190043Z","iopub.status.idle":"2023-06-12T15:13:51.197498Z","shell.execute_reply.started":"2023-06-12T15:13:51.190013Z","shell.execute_reply":"2023-06-12T15:13:51.196523Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"learning_rate_exploration(train_X, train_y, test_X, test_y)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T19:37:43.323657Z","iopub.execute_input":"2023-06-08T19:37:43.324582Z","iopub.status.idle":"2023-06-08T19:39:05.292108Z","shell.execute_reply.started":"2023-06-08T19:37:43.324530Z","shell.execute_reply":"2023-06-08T19:39:05.291016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TODO : Write a function which iterates over different depths, \n## train a classifier with a specified depth and print accuracy, precision, and recall\n## Set trees_amount= 50 to make the calculations faster\n## Note: the calculations may take several minutes (depending on the computer efficiency).\n\ndef max_depth_exploration(train_X, train_y, test_X, test_y, depths=[1,2,3,5]):\n    # TODO\n     for depth in depths:\n        model = GradientBoostingClassifier(max_depth=depth)\n        model.fit(train_X, train_y)\n        \n        predictions = model.predict(test_X)\n        accuracy = accuracy_score(test_y, predictions)\n        precision = precision_score(test_y, predictions, average='weighted')\n        recall = recall_score(test_y, predictions, average='weighted')\n        \n        print('Depth:', depth, '\\n')\n        \n        print('Accuracy:', accuracy)\n        print('Precision:', precision)\n        print('Recall:', recall, '\\n \\n')","metadata":{"id":"T-akqJfbEkVF","execution":{"iopub.status.busy":"2023-06-12T15:13:54.032529Z","iopub.execute_input":"2023-06-12T15:13:54.032917Z","iopub.status.idle":"2023-06-12T15:13:54.042301Z","shell.execute_reply.started":"2023-06-12T15:13:54.032885Z","shell.execute_reply":"2023-06-12T15:13:54.041256Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"max_depth_exploration(train_X, train_y, test_X, test_y)","metadata":{"id":"3oFAnFOnEkdX","execution":{"iopub.status.busy":"2023-06-08T19:41:47.891696Z","iopub.execute_input":"2023-06-08T19:41:47.892718Z","iopub.status.idle":"2023-06-08T19:42:47.183415Z","shell.execute_reply.started":"2023-06-08T19:41:47.892672Z","shell.execute_reply":"2023-06-08T19:42:47.182254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TODO :**   \n\n* How does the max_depth parameter influence the results? \n* How does the learning rate influence the results?\n* How does the number of trees in the ensemble influence the results?\n* Try to improve the accuracy by combining different max_depth, learning rate and number of trees. How well does your best model perform?","metadata":{"id":"75fc86d1"}},{"cell_type":"code","source":"## TODO -  sklearn trees have the attribute feature_importances_\n## make a plot, to show relative importance (maximum is 1) of your classifier and\n## order features from most relevant feature to the least relevant in the plot\n\ndef plot_explained_variance(clf, X):\n    # TODO","metadata":{"id":"aa0c330e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TODO : display the plot\n","metadata":{"id":"6cf90239"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TODO :** Interpret the plot.\n\n**TODO (optional):** Try to remove the least-important features and see what happens. Does to quality improve or degrade? Why? ","metadata":{"id":"2e0872d8"}},{"cell_type":"markdown","source":"## Prepare for deep learning\n### Add all the necessary training functions \n*You can reuse them from previous practical exercises*","metadata":{"id":"jhE75y_UN7Qx"}},{"cell_type":"code","source":"## TODO write a function that calculates the accuracy\n## Hint - you can use yours from practical 3 \n\ndef accuracy(correct, total): \n    \"\"\"\n    function to calculate the accuracy given the\n        correct: number of correctly classified samples\n        total: total number of samples\n    returns the ratio\n    \"\"\"\n    ratio = 100 * (correct.item())/total \n    return ratio\n    ","metadata":{"id":"LfDLPO2QODES","execution":{"iopub.status.busy":"2023-06-12T16:04:13.768172Z","iopub.execute_input":"2023-06-12T16:04:13.768621Z","iopub.status.idle":"2023-06-12T16:04:13.786816Z","shell.execute_reply.started":"2023-06-12T16:04:13.768577Z","shell.execute_reply":"2023-06-12T16:04:13.778331Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"## TODO : Define a train and validation functions here\n## Hint - you can use yours from practical 3 \n\ndef train(dataloader, optimizer, model, loss_function):\n    \"\"\" method to train the model \"\"\"\n\n    # TODO: refine the training function from above\n  # it should contain:\n  # - saving of losses\n  # - calculation of accuracy\n  # - returning the mean loss and accuracy\n    losses = []\n    model.train()\n    sum_loss = 0.0\n    total = 0\n    correct = 0\n    \n    \n    for i, (x,y) in enumerate(dataloader):\n        x= x.to(device)\n        y = y.to(device).squeeze()\n                        \n        optimizer.zero_grad()\n        \n        y_pred = model(x)\n        _, predicted = torch.max(y_pred.data,1)\n        \n\n        # Compute loss\n        loss = loss_function(y_pred, y)\n        \n\n        # Backward pass -> calculate gradients, update weights\n        loss.backward()\n        optimizer.step()\n        \n        losses.append(loss.item())\n        total += y.size(0)\n        sum_loss += loss.item()\n        correct += (predicted == y).sum()\n    \n    n = len(losses)\n    mean_loss= sum_loss/n\n    acc = accuracy(correct, total)\n    \n    return mean_loss, acc\n\n\ndef validate(dataloader, model, loss_fn, device):\n    \"\"\" method to compute the metrics on the validation set \"\"\"\n    # Todo\n    model.eval()  # Set the model in evaluation mode\n    val_loss = 0.0\n    correct = 0\n    total=0\n    \n    with torch.no_grad():\n        for i, (x_val, y_val) in enumerate(dataloader):\n            x_val = x_val.to(device)\n            y_val = y_val.to(device)\n\n            y_pred_val = model(x_val)\n            _, predicted = torch.max(y_pred_val.data,1)\n            loss_val = loss_function(y_pred_val.squeeze(), y_val)     \n\n            val_loss += loss_val.item()\n            total += y_val.size(0)\n            correct += ( predicted == y_val).sum()\n\n        val_loss /= len(dataloader)\n        acc = accuracy(correct, total)\n    \n    return val_loss, acc\n    ","metadata":{"id":"EVe7SFnrODHP","execution":{"iopub.status.busy":"2023-06-12T16:11:00.983140Z","iopub.execute_input":"2023-06-12T16:11:00.983508Z","iopub.status.idle":"2023-06-12T16:11:00.996122Z","shell.execute_reply.started":"2023-06-12T16:11:00.983482Z","shell.execute_reply":"2023-06-12T16:11:00.995051Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#TODO write a run_training function that \n# - calls the train and validate functions for each epoch\n# - saves the train_losses, val_losses, train_accs, val_accs as arrays for each epoch\n## Hint - you can use yours from practical 3 \nfrom tqdm import tqdm\n\n\ndef run_training(model, optimizer, loss_function, device, num_epochs, train_dataloader, val_dataloader):\n    \"\"\" method to run the training procedure \"\"\"\n    # TODO\n    train_losses=[]\n    val_losses = []\n    train_accs = []\n    val_accs = []\n    for epoch in tqdm(range(int(num_epochs)), desc = 'Training Epochs'):\n        train_loss=train(train_dataloader, optimizer, model, loss_function)[0]\n        train_acc= train(train_dataloader, optimizer, model, loss_function)[1]\n        val_loss = validate(test_dataloader,model,loss_function,device)[0]\n        val_acc= validate(test_dataloader,model,loss_function,device)[1]\n            \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        train_accs.append(train_acc)\n        val_accs.append(val_acc)\n        \n        print('Epoch: {}. Loss: {}. Accuracy: {}'.format(epoch, train_loss, train_acc))\n        \n    return train_losses, val_losses, train_accs, val_accs\n        ","metadata":{"id":"pzGM0bPqxw3V","execution":{"iopub.status.busy":"2023-06-12T16:04:18.875538Z","iopub.execute_input":"2023-06-12T16:04:18.875883Z","iopub.status.idle":"2023-06-12T16:04:18.887816Z","shell.execute_reply.started":"2023-06-12T16:04:18.875856Z","shell.execute_reply":"2023-06-12T16:04:18.886878Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# TODO write a plot function \n## Hint - you can use yours from practical 2 or 3 #\n\n","metadata":{"id":"ngflrAEJxw5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert a pandas dataframe to a PyTorch dataset","metadata":{"id":"9iqaRi2xRADg"}},{"cell_type":"code","source":"## TODO : Define the dataset, apply normalization in the getitem method\n## Hint : you can use/adapt your code from practical 2\nclass TabularDataset(torch.utils.data.Dataset):\n    def __init__(self, df_x, df_y, mean=None, std=None, normalise=True):\n        '''\n        TODO: save params to self attributes, \n        x is data without target column\n        y is target column\n        transform df to_numpy\n        ''' \n        self.x = torch.tensor(df_x.values, dtype=torch.float32)\n        self.y = torch.tensor(df_y.values, dtype=torch.float32)\n        self.mean = mean\n        self.std = std\n        self.normalise = normalise\n    \n    def __len__(self):\n        # TODO: return the length of the whole dataset\n        return len(self.y)\n\n    \n    def __getitem__(self, index):\n        ## TODO: return X, y by index, normalized if needed\n        X = self.x[index]\n        y = self.y[index]\n        if self.normalise and self.mean is not None and self.std is not None:\n            X = (X- self.mean) / self.std\n        return X,y","metadata":{"id":"e3dcbc2c","execution":{"iopub.status.busy":"2023-06-12T16:04:22.560247Z","iopub.execute_input":"2023-06-12T16:04:22.560587Z","iopub.status.idle":"2023-06-12T16:04:22.570705Z","shell.execute_reply.started":"2023-06-12T16:04:22.560560Z","shell.execute_reply":"2023-06-12T16:04:22.569679Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"## TODO : calculate mean and std for the train set\n## Hint : be careful with categorical values. Convert them them to numerical \n## Hint : the response variable should be of datatype integer\n\ntrain_y = pd.get_dummies(train_y)\ntest_y = pd.get_dummies(test_y)\n\ncat_columns = train_X.select_dtypes(['category']).columns\ntrain_X[cat_columns] = train_X[cat_columns].apply(lambda x: x.cat.codes)\n\ntmp_dataset = TabularDataset(train_X, train_y, normalise= False)\n\nmean = torch.mean(tmp_dataset.x, axis=0)\nstd = torch.std(tmp_dataset.x, axis=0)\n\nprint(mean)\nprint(std)\n        ","metadata":{"id":"3jJXxpF-KIpz","execution":{"iopub.status.busy":"2023-06-12T16:04:25.447653Z","iopub.execute_input":"2023-06-12T16:04:25.448016Z","iopub.status.idle":"2023-06-12T16:04:25.555758Z","shell.execute_reply.started":"2023-06-12T16:04:25.447973Z","shell.execute_reply":"2023-06-12T16:04:25.554773Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"tensor([ 9.4492, 11.9831,  0.0555,  0.2947,  0.3868,  6.9186, 20.2898,  1.9120,\n         1.4448,  6.4592,  0.2666,  0.1767,  7.8492,  4.8346])\ntensor([ 5.8374,  5.9860,  0.6357,  1.5907,  1.8041,  1.7337, 17.1985,  1.3433,\n         0.8988,  2.1332,  1.2212,  0.9777,  6.2523,  2.8173])\n","output_type":"stream"}]},{"cell_type":"code","source":"# TODO : define new datasets with mean, std and normalise=True\n# be careful with the labels, they should start from 0!\nconductor_train = TabularDataset(train_X, train_y, mean=mean, std=std, normalise=True)\nconductor_test = TabularDataset(test_X, test_y,mean=mean,std=std, normalise=True)\n\n## TODO : define dataloaders, with specified batch size and shuffled\n\nbatch_size = 256\ntrain_dataloader = DataLoader(conductor_train,batch_size=batch_size, shuffle=True)\ntest_dataloader =DataLoader(conductor_test,batch_size=batch_size, shuffle=True)\n","metadata":{"id":"Ef6dNkihxzYd","execution":{"iopub.status.busy":"2023-06-12T16:04:29.373503Z","iopub.execute_input":"2023-06-12T16:04:29.373906Z","iopub.status.idle":"2023-06-12T16:04:29.386366Z","shell.execute_reply.started":"2023-06-12T16:04:29.373871Z","shell.execute_reply":"2023-06-12T16:04:29.385235Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Logistic regression","metadata":{"id":"N28MlXQ7ON1j"}},{"cell_type":"code","source":"class LR(torch.nn.Module):\n    \"\"\"\n    The logistic regression model inherits from torch.nn.Module \n    which is the base class for all neural network modules.\n    \"\"\"\n    def __init__(self, input_dim, output_dim):\n        \"\"\" Initializes internal Module state. \"\"\"\n        super(LR, self).__init__()\n        # TODO define linear layer for the model\n        self.linear = nn.Linear(input_dim, output_dim)\n        \n\n    def forward(self, x):\n        \"\"\" Defines the computation performed at every call. \"\"\"\n        # What are the dimensions of your input layer?\n        x = x.to(torch.float32)\n        #x = x.view(x.size(0), -1)\n        # TODO run the data through the layer\n        outputs = torch.sigmoid(self.linear(x))\n        \n        return outputs","metadata":{"id":"NT-EtgZKOLb0","execution":{"iopub.status.busy":"2023-06-12T16:11:11.118911Z","iopub.execute_input":"2023-06-12T16:11:11.119621Z","iopub.status.idle":"2023-06-12T16:11:11.127462Z","shell.execute_reply.started":"2023-06-12T16:11:11.119578Z","shell.execute_reply":"2023-06-12T16:11:11.126506Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"## TODO define model, loss and optimisers\n## don't forget to move everything for the correct devices\n## \nlr=0.001\noutput_dim = 1\ninput_dim = train_X.shape[1]\nlinear_regression = LR(input_dim, output_dim)\nlinear_regression.to(device)\nlinear_regression.train()\n\noptimizer = torch.optim.Adam(linear_regression.parameters(), lr=lr)\nloss_function = nn.CrossEntropyLoss()\n\n","metadata":{"id":"eWDd_hYGOZSj","execution":{"iopub.status.busy":"2023-06-12T16:11:14.234415Z","iopub.execute_input":"2023-06-12T16:11:14.235224Z","iopub.status.idle":"2023-06-12T16:11:14.242384Z","shell.execute_reply.started":"2023-06-12T16:11:14.235190Z","shell.execute_reply":"2023-06-12T16:11:14.241195Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"## TODO train the network\nnum_epochs = 30\nrun_training(linear_regression, optimizer,loss_function, device, num_epochs, train_dataloader, test_dataloader)","metadata":{"id":"6EFwKn-iOgNv","execution":{"iopub.status.busy":"2023-06-12T16:11:22.244145Z","iopub.execute_input":"2023-06-12T16:11:22.245229Z","iopub.status.idle":"2023-06-12T16:11:22.390230Z","shell.execute_reply.started":"2023-06-12T16:11:22.245188Z","shell.execute_reply":"2023-06-12T16:11:22.388596Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Training Epochs:   0%|          | 0/30 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## TODO train the network\u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinear_regression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(model, optimizer, loss_function, device, num_epochs, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     14\u001b[0m val_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(num_epochs)), desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Epochs\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     train_loss\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     17\u001b[0m     train_acc\u001b[38;5;241m=\u001b[39m train(train_dataloader, optimizer, model, loss_function)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     18\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate(test_dataloader,model,loss_function,device)[\u001b[38;5;241m0\u001b[39m]\n","Cell \u001b[0;32mIn[20], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, optimizer, model, loss_function)\u001b[0m\n\u001b[1;32m     26\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(y_pred\u001b[38;5;241m.\u001b[39mdata,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Backward pass -> calculate gradients, update weights\u001b[39;00m\n\u001b[1;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"],"ename":"RuntimeError","evalue":"0D or 1D target tensor expected, multi-target not supported","output_type":"error"}]},{"cell_type":"code","source":"## todo - plot losses and accuracies\nplot('Epoch vs. Loss', 'Loss', train_losses_lr, val_losses_lr)","metadata":{"id":"H-IJWOsATYul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot('Epoch vs. Accuracy', 'Accuracy', train_accs_lr, val_accs_lr)","metadata":{"id":"HjAaYdd8TZed"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Create a simple MLP\n\nAs the default tree has 3 layers, let's make a MLP with 3 linear layers and ReLU.\nPlease notice that making convolutions on tabular data does not make much sense even though it is technically possible.   \n\n**TODO :** Explain why making convolutions on tabular data does not make much sense. Why do we use an MLP, not a CNN from the previous homework?","metadata":{"id":"cb1bca9a"}},{"cell_type":"code","source":"class TabularNetwork(torch.nn.Module):\n    def __init__(self, input_dim, output_dim):\n        \"\"\" Initializes internal Module state. \"\"\"\n        super(TabularNetwork, self).__init__()\n        self.network = nn.Sequential(\n            # TODO : define 3 linear layer with sizes \n            # input_dim -> input_dim // 2 -> output_dim\n            # using ReLU as nonlinearity\n            \n        )\n      \n\n    def forward(self, x):\n        \"\"\" Defines the computation performed at every call. \"\"\"\n        # TODO","metadata":{"id":"4eecbb52"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TODO : define model, optimiser, cross entropy loss,\n## put model to the device, and train mode\n## you can optionally apply regularisation between 0.0005 and 0.005 \nlr=0.001\n","metadata":{"id":"5OGid8SpMvZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TODO : Train model\nnum_epochs = 50\n","metadata":{"id":"Z-WNxtczM5Iv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO plot losses\nplot('Epoch vs. Loss', 'Loss', train_losses, val_losses)","metadata":{"id":"I_xVns4qM5eO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO plot accuracies\nplot('Epoch vs. Accuracy', 'Accuracy', train_accs, val_accs)","metadata":{"id":"JRAEaYnYUpd_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TODO:** Did your network perform better or worse than the GradientBoostingClassifier on this dataset? Why? \n","metadata":{"id":"J8HbVqiUznBu"}},{"cell_type":"markdown","source":"## Bonus tasks (optional)\n* Try to use SGD instead of Adam as optimiser. What do you notice?\nHere are different opinions on this topic:\n  * https://codeahoy.com/questions/ds-interview/33/#:~:text=Adam%20tends%20to%20converge%20faster,converges%20to%20more%20optimal%20solutions.\n  * https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/ \n  * https://datascience.stackexchange.com/questions/30344/why-not-always-use-the-adam-optimization-technique\n\n* Try to make your MLP twice deeper. What do you notice? Why?\n\n## Advanced topic to read about:\n**Tools which may be helpful for data exploration:**\n* df.describe() - returns some basic statistics for your dataset - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html\n* ydata-profiling (previous pandas-profiling) - generates interactive data exploration report: basic statistics, nans, correlations between different features - https://github.com/ydataai/ydata-profiling\n\n**Tree libraries**\n* XGBoost - XGBoost stands for “Extreme Gradient Boosting”, where the term “Gradient Boosting” originates from the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. https://xgboost.readthedocs.io/en/stable/tutorials/model.html\n* LightGBM - industrial library for XGBoost from Miscrosoft. LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient. https://lightgbm.readthedocs.io/en/v3.3.2/","metadata":{"id":"cc0f5f87"}},{"cell_type":"markdown","source":"","metadata":{}}]}